{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b317257",
   "metadata": {},
   "source": [
    "# 3DDFA Evaluation on 300W-LP\n",
    "\n",
    "This notebook evaluates a 3DDFA model (outputs 3DMM parameters) on the **300W-LP** dataset using **ONNX Runtime (GPU)**. Metrics included:\n",
    "\n",
    "- NME (Normalized Mean Error) — primary metric, normalized by inter-ocular distance\n",
    "- RMSE (pixel-space)\n",
    "- MAE (pixel-space)\n",
    "- FPS (inference speed)\n",
    "- Chamfer Distance (optional, if dense ground-truth meshes available)\n",
    "\n",
    "**Before running:** place your ONNX model, BFM `.pkl`, and the 300W-LP dataset in accessible paths and update the configuration cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install (uncomment and run if needed)\n",
    "# !pip install onnxruntime-gpu==1.15.1 numpy scipy matplotlib tqdm imageio opencv-python trimesh==3.21.11\n",
    "# If you don't have GPU or onnxruntime-gpu, use onnxruntime (CPU): !pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: edit these paths before running\n",
    "ONNX_MODEL_PATH = 'models/3ddfa.onnx'   # path to your 3DDFA ONNX model that outputs 3DMM params\n",
    "BFM_PKL_PATH = 'bfm/bfm_noneck_v3.pkl' # path to BFM pkl used by your model (matching training)\n",
    "DATASET_ROOT = 'datasets/300W_LP'      # root dir for 300W-LP images and annotations\n",
    "USE_GPU = True                         # set False to use CPU provider\n",
    "NUM_WORKERS = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc88e5",
   "metadata": {},
   "source": [
    "## Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ONNX runtime\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Optional: trimesh for Chamfer (if ground-truth dense meshes exist)\n",
    "try:\n",
    "    import trimesh\n",
    "    HAS_TRIMESH = True\n",
    "except Exception as e:\n",
    "    HAS_TRIMESH = False\n",
    "    print('trimesh not available - Chamfer will be skipped if requested')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0556a1dd",
   "metadata": {},
   "source": [
    "## Helpers: load BFM (3DMM) and reconstruct vertices from parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4615f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bfm(bfm_pkl_path):\n",
    "    with open(bfm_pkl_path, 'rb') as f:\n",
    "        bfm = pickle.load(f, encoding='latin1') if ('rb' in f.mode) else pickle.load(f)\n",
    "    u = bfm.get('u').astype(np.float32)\n",
    "    w_shp = bfm.get('w_shp').astype(np.float32)\n",
    "    w_exp = bfm.get('w_exp').astype(np.float32)\n",
    "    tri = bfm.get('tri')\n",
    "    keypoints = bfm.get('keypoints', None)\n",
    "    return {'u': u, 'w_shp': w_shp, 'w_exp': w_exp, 'tri': tri, 'keypoints': keypoints}\n",
    "\n",
    "def reconstruct_vertices(bfm, alpha_shp, alpha_exp, R=None, offset=None):\n",
    "    \"\"\"Reconstruct dense vertices from 3DMM parameters.\n",
    "    alpha_shp: (shape_dim,) or (shape_dim,1)\n",
    "    alpha_exp: (exp_dim,) or (exp_dim,1)\n",
    "    \"\"\"\n",
    "    u = bfm['u']\n",
    "    w_shp = bfm['w_shp']\n",
    "    w_exp = bfm['w_exp']\n",
    "\n",
    "    # Ensure shapes: convert 2D->(N,3,k) if stored flattened\n",
    "    if w_shp.ndim == 2:\n",
    "        sd = alpha_shp.shape[0]\n",
    "        w_shp = w_shp.reshape(-1, 3, sd)\n",
    "    if w_exp.ndim == 2:\n",
    "        ed = alpha_exp.shape[0]\n",
    "        w_exp = w_exp.reshape(-1, 3, ed)\n",
    "\n",
    "    delta_shp = np.tensordot(w_shp, alpha_shp.reshape(-1), axes=([2],[0]))\n",
    "    delta_exp = np.tensordot(w_exp, alpha_exp.reshape(-1), axes=([2],[0]))\n",
    "    verts = u + delta_shp + delta_exp\n",
    "\n",
    "    if R is not None:\n",
    "        verts = verts @ R.T\n",
    "    if offset is not None:\n",
    "        verts = verts + offset.reshape(1,3)\n",
    "    return verts.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da073b83",
   "metadata": {},
   "source": [
    "## ONNX Runtime: create session (GPU if available) and inference wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ort_session(onnx_path, use_gpu=True):\n",
    "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu else ['CPUExecutionProvider']\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    sess = ort.InferenceSession(onnx_path, sess_options, providers=providers)\n",
    "    print('ONNX session providers:', sess.get_providers())\n",
    "    return sess\n",
    "\n",
    "def run_inference(sess, img):\n",
    "    \"\"\"Run model on a single image. Update preprocessing/output parsing for your model.\"\"\"\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    H, W = img.shape[:2]\n",
    "    img_resized = cv2.resize(img, (224, 224))\n",
    "    img_trans = img_resized.astype(np.float32) / 255.0\n",
    "    img_trans = np.transpose(img_trans, (2,0,1))[None, ...]\n",
    "    outputs = sess.run(None, {input_name: img_trans})\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08666755",
   "metadata": {},
   "source": [
    "## Metric functions: NME, RMSE, MAE, Chamfer (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071114ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inter_ocular_distance(gt_landmarks):\n",
    "    left = gt_landmarks[36]\n",
    "    right = gt_landmarks[45]\n",
    "    return np.linalg.norm(left - right)\n",
    "\n",
    "def compute_nme(pred, gt, normalize_by='interocular'):\n",
    "    pred2 = pred[:, :2]\n",
    "    gt2 = gt[:, :2]\n",
    "    d = inter_ocular_distance(gt2) if normalize_by=='interocular' else np.linalg.norm(gt2.max(axis=0)-gt2.min(axis=0))\n",
    "    nme = np.mean(np.linalg.norm(pred2-gt2, axis=1)) / d\n",
    "    return nme\n",
    "\n",
    "def compute_rmse(pred, gt):\n",
    "    return np.sqrt(np.mean(np.sum((pred[:,:2]-gt[:,:2])**2, axis=1)))\n",
    "\n",
    "def compute_mae(pred, gt):\n",
    "    return np.mean(np.abs(pred[:,:2]-gt[:,:2]))\n",
    "\n",
    "def chamfer_distance(pc1, pc2):\n",
    "    if not HAS_TRIMESH:\n",
    "        raise RuntimeError('trimesh not available')\n",
    "    from scipy.spatial import cKDTree\n",
    "    tree1 = cKDTree(pc1)\n",
    "    tree2 = cKDTree(pc2)\n",
    "    d1,_ = tree1.query(pc2)\n",
    "    d2,_ = tree2.query(pc1)\n",
    "    return np.mean(d1) + np.mean(d2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cc5d4",
   "metadata": {},
   "source": [
    "## Dataset loader (300W-LP) — adjust to your annotation format\n",
    "\n",
    "This cell shows an example loader assuming annotations contain 2D landmarks per image. Edit as needed to match your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_300w_lp_annotations(root_dir):\n",
    "    samples = []\n",
    "    ann_dir = Path(root_dir) / 'annotations'\n",
    "    img_dir = Path(root_dir) / 'images'\n",
    "    for ann_fp in ann_dir.glob('*.pts'):\n",
    "        img_fp = img_dir / (ann_fp.stem + '.jpg')\n",
    "        try:\n",
    "            pts = np.loadtxt(ann_fp)\n",
    "        except Exception:\n",
    "            continue\n",
    "        samples.append({'image': str(img_fp), 'landmarks': pts})\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843c469",
   "metadata": {},
   "source": [
    "## Evaluation loop\n",
    "This cell runs inference on the dataset and computes metrics. It also measures FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b84482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(onnx_path, bfm_pkl, dataset_root, use_gpu=True, max_samples=None):\n",
    "    sess = make_ort_session(onnx_path, use_gpu)\n",
    "    bfm = load_bfm(bfm_pkl)\n",
    "    samples = load_300w_lp_annotations(dataset_root)\n",
    "\n",
    "    nmEs = []\n",
    "    rmses = []\n",
    "    maes = []\n",
    "    times = []\n",
    "\n",
    "    for i, s in enumerate(tqdm(samples)):\n",
    "        if max_samples and i>=max_samples:\n",
    "            break\n",
    "        img = cv2.imread(s['image'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        t0 = time.time()\n",
    "        outputs = run_inference(sess, img)\n",
    "        t1 = time.time()\n",
    "        times.append(t1-t0)\n",
    "\n",
    "        # Parse outputs depending on your ONNX model outputs\n",
    "        # Example assume outputs = [pose_vec, alpha_shp, alpha_exp]\n",
    "        try:\n",
    "            pose_vec = outputs[0].reshape(-1)\n",
    "            alpha_shp = outputs[1].reshape(-1)\n",
    "            alpha_exp = outputs[2].reshape(-1)\n",
    "        except Exception:\n",
    "            print('Model outputs:', [o.name for o in sess.get_outputs()])\n",
    "            raise\n",
    "\n",
    "        verts = reconstruct_vertices(bfm, alpha_shp, alpha_exp)\n",
    "\n",
    "        if 'landmarks' in s:\n",
    "            gt = s['landmarks']\n",
    "            if bfm.get('keypoints') is not None:\n",
    "                kps = np.array(bfm['keypoints']).astype(int)\n",
    "                pred_landmarks_3d = verts[kps]\n",
    "                pred_landmarks_2d = pred_landmarks_3d[:,:2]\n",
    "            else:\n",
    "                pred_landmarks_2d = verts[:len(gt), :2]\n",
    "\n",
    "            nme = compute_nme(pred_landmarks_2d, gt)\n",
    "            rmse = compute_rmse(pred_landmarks_2d, gt)\n",
    "            mae = compute_mae(pred_landmarks_2d, gt)\n",
    "\n",
    "            nmEs.append(nme)\n",
    "            rmses.append(rmse)\n",
    "            maes.append(mae)\n",
    "\n",
    "    results = {\n",
    "        'NME_mean': float(np.mean(nmEs)) if len(nmEs)>0 else None,\n",
    "        'NME_median': float(np.median(nmEs)) if len(nmEs)>0 else None,\n",
    "        'RMSE_mean': float(np.mean(rmses)) if len(rmses)>0 else None,\n",
    "        'MAE_mean': float(np.mean(maes)) if len(maes)>0 else None,\n",
    "        'FPS': float(1.0/np.mean(times)) if len(times)>0 else None,\n",
    "        'N_samples': len(nmEs)\n",
    "    }\n",
    "    return results, nmEs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run (uncomment to execute):\n",
    "# results, nmEs = evaluate(ONNX_MODEL_PATH, BFM_PKL_PATH, DATASET_ROOT, use_gpu=USE_GPU, max_samples=200)\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b8692",
   "metadata": {},
   "source": [
    "## Visualization helpers\n",
    "Plot NME CDF and example overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b349a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nme_cdf(nmes, ax=None):\n",
    "    import numpy as np\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    nmes = np.array(nmes)\n",
    "    vals = np.sort(nmes)\n",
    "    cdf = np.arange(1, len(vals)+1) / len(vals)\n",
    "    ax.plot(vals, cdf)\n",
    "    ax.set_xlabel('NME')\n",
    "    ax.set_ylabel('CDF')\n",
    "    ax.grid(True)\n",
    "    return ax\n",
    "\n",
    "# Example usage:\n",
    "# ax = plot_nme_cdf(nmEs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0659785",
   "metadata": {},
   "source": [
    "## Notes and Interpretation\n",
    "\n",
    "- **NME**: primary metric. Lower is better. Use inter-ocular normalization.\n",
    "- **RMSE/MAE**: raw pixel errors — useful for intuition.\n",
    "- **FPS**: shows inference speed; measure on target hardware.\n",
    "- **Chamfer**: if you have dense GT meshes, Chamfer measures 3D similarity.\n",
    "\n",
    "Interpretation guidance:\n",
    "\n",
    "- Report mean and median NME; median is robust to outliers.\n",
    "- Provide CDF plot of NME (x-axis NME, y-axis proportion ≤ that NME). Many papers show the curve.\n",
    "- Compare FPS separately (model accuracy vs speed tradeoff).\n",
    "\n",
    "Edit dataset loading and model input/output parsing to match your exact ONNX model and dataset format."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
